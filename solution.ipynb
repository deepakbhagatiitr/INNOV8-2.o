{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Step 1: Import Necessary Libraries\n","\n","- **`import pandas as pd`**: This imports the Pandas library, which is used to handle and analyze data in the form of tables.\n","\n","- **`from sklearn.model_selection import train_test_split`**: This function is used to split your data into two parts: one for training the model and one for testing it.\n","\n","- **`from sklearn.feature_extraction.text import TfidfVectorizer`**: This converts text data into numerical form (using TF-IDF), making it easier for the model to understand.\n","\n","- **`from sklearn.preprocessing import LabelEncoder, StandardScaler`**:\n","  - **`LabelEncoder`**: Converts categorical labels (like species names) into numbers.\n","  - **`StandardScaler`**: Scales numerical data to make sure all values are in a similar range.\n","\n","- **`from sklearn.linear_model import LogisticRegression`**: This is the Logistic Regression model used to classify data.\n","\n","- **`from sklearn.metrics import accuracy_score`**: This calculates how accurate your model's predictions are compared to the correct answers.\n","\n","- **`import joblib`**: This saves or loads the trained model, so you can reuse it later.\n","\n","- **`import numpy as np`**: This imports NumPy, a library used for handling arrays and numerical data.\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","import joblib\n","import numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["Step 2: Load the dataset\n","### Loading a CSV File into a DataFrame\n","\n","This line of code loads data from a CSV file into a Pandas DataFrame:\n","\n","The `pd.read_csv('data.csv')` function is used to read the contents of a CSV file. In this case, `'data.csv'` is the file path provided.\n","\n","- **Pandas DataFrame**: A DataFrame is a two-dimensional, size-mutable, and heterogeneous tabular data structure with labeled axes (rows and columns).\n","- **CSV File**: A comma-separated values (CSV) file stores tabular data in plain text.\n","\n","Once the CSV file is loaded, the resulting DataFrame can be used for various operations, such as data manipulation, analysis, and visualization.\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["data = pd.read_csv('data.csv')"]},{"cell_type":"markdown","metadata":{},"source":["Step 3: Data Preprocessing\n","### Converting Categorical Values to Binary\n","\n","In this step, we are transforming the values in the `tail` column of the `data` DataFrame:\n","\n","- **Purpose**: \n","  - The goal is to convert categorical responses (in this case, 'Yes' and 'No') into binary values (1 and 0). This is useful for machine learning models that require numeric input.\n","\n","- **Code Explanation**:\n","  - The code `data['tail'] = data['tail'].apply(lambda x: 1 if x == 'Yes' else 0)` uses the `apply()` function along with a lambda expression.\n","  - **Lambda Function**: \n","    - The lambda function checks each value `x` in the `tail` column.\n","    - If `x` is equal to `'Yes'`, it assigns a value of `1`.\n","    - For any other value, it assigns a value of `0`.\n","\n","This conversion allows the `tail` column to be used as a numeric feature in machine learning algorithms, enhancing the model's ability to learn from the data.\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["data['tail'] = data['tail'].apply(lambda x: 1 if x == 'Yes' else 0)"]},{"cell_type":"markdown","metadata":{},"source":["### Label Encoding Categorical Variables\n","\n","In this step, we are using `LabelEncoder` to convert categorical data in the `species` column into numeric values:\n","\n","- **LabelEncoder**: \n","  - A utility from `sklearn.preprocessing` that encodes categorical labels into integers.\n","  \n","- **Purpose**: \n","  - Machine learning models work better with numeric data, so we convert text labels into numbers.\n","  \n","- **fit_transform()**:\n","  - **fit**: The encoder learns the unique categories in the `species` column.\n","  - **transform**: Each category is replaced with a corresponding numeric label.\n","  \n","For example, if the `species` column contains categories like `'setosa'`, `'versicolor'`, and `'virginica'`, the encoder will map them to `0`, `1`, and `2`, respectively. This makes the categorical data usable for machine learning models.\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["le = LabelEncoder()\n","data['species'] = le.fit_transform(data['species'])"]},{"cell_type":"markdown","metadata":{},"source":["### Separating Features and Target Variable\n","\n","In this step, we are splitting the dataset into features (`X`) and the target variable (`y`):\n","\n","- **Features (`X`)**: \n","  - The columns `['message', 'fingers', 'tail']` are selected from the `data` dataframe as the input features.\n","  - These are the independent variables used to make predictions.\n","\n","- **Target (`y`)**: \n","  - The `species` column is chosen as the target variable, which is what we are trying to predict.\n","  - This is the dependent variable.\n","\n","By separating the features and target, we prepare the data for machine learning algorithms, which use the features (`X`) to learn patterns and make predictions on the target (`y`).\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["X = data[['message', 'fingers', 'tail']]\n","y = data['species']"]},{"cell_type":"markdown","metadata":{},"source":["### Splitting the Data into Training and Testing Sets\n","\n","This step involves dividing the dataset into training and testing subsets using the `train_test_split()` function:\n","\n","- **Features and Target**: \n","  - `X` represents the features (input variables).\n","  - `y` represents the target variable (what we are predicting).\n","  \n","- **train_test_split()**: \n","  - The `train_test_split()` function is used to randomly split the data into training and testing sets.\n","  \n","- **Parameters**:\n","  - `test_size=0.2`: This means 20% of the data will be used for testing, and 80% will be used for training.\n","  - `random_state=42`: The random state ensures reproducibility of the split, meaning the same data split will occur each time the code is run.\n","  \n","The resulting variables:\n","- `X_train` and `y_train`: Used to train the model.\n","- `X_test` and `y_test`: Used to evaluate the model's performance on unseen data."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"]},{"cell_type":"markdown","metadata":{},"source":["Step 4: Text vectorization (TF-IDF)\n","### Applying TF-IDF Vectorization\n","\n","In this step, we are using the `TfidfVectorizer` to transform the text data into numerical feature vectors:\n","\n","- **TF-IDF (Term Frequency-Inverse Document Frequency)**: \n","  - This method converts text data into numerical values, giving higher importance to words that are frequent in a document but less frequent across all documents. \n","  - It is useful for extracting important features from textual data.\n","\n","- **TfidfVectorizer(max_features=500)**: \n","  - `max_features=500` limits the number of features (words) to 500. This helps reduce noise by selecting the top 500 most relevant words based on their TF-IDF scores.\n","  - This is an important step in text preprocessing, especially when dealing with large text datasets, to improve the model's performance and reduce overfitting.\n","\n","By limiting the features, we ensure that only the most significant words contribute to the model."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["tfidf = TfidfVectorizer(max_features=500)"]},{"cell_type":"markdown","metadata":{},"source":["### Transforming Text Data Using TF-IDF\n","\n","In this step, we are applying the TF-IDF transformation to the training data:\n","\n","- **Purpose**: \n","  - This transformation converts the text data in the `message` column of the `X_train` DataFrame into a numerical format that can be used for machine learning algorithms.\n","\n","- **Code Explanation**:\n","  - The code `X_train_tfidf = tfidf.fit_transform(X_train['message'])` does the following:\n","    - **fit_transform()**:\n","      - **fit**: The TF-IDF vectorizer learns the vocabulary from the training data (i.e., it identifies the unique words and calculates their frequencies).\n","      - **transform**: It then transforms the text in the `message` column into a TF-IDF representation, producing a sparse matrix of shape `(number of samples, number of features)`.\n","    - The resulting variable, `X_train_tfidf`, contains the TF-IDF values for each message in the training set, where each row corresponds to a message, and each column represents a feature derived from the vocabulary.\n","\n","This numerical representation of text allows machine learning models to analyze and learn from the textual data effectively.\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["X_train_tfidf = tfidf.fit_transform(X_train['message'])"]},{"cell_type":"markdown","metadata":{},"source":["### Transforming Test Data Using TF-IDF\n","\n","In this step, we are applying the TF-IDF transformation to the test data:\n","\n","- **Purpose**: \n","  - This transformation converts the text data in the `message` column of the `X_test` DataFrame into a numerical format, using the vocabulary learned from the training data.\n","\n","- **Code Explanation**:\n","  - The code `X_test_tfidf = tfidf.transform(X_test['message'])` does the following:\n","    - **transform()**: \n","      - This method applies the TF-IDF vectorizer to the test data without re-fitting it. It uses the same vocabulary and IDF values learned from the training data.\n","    - The resulting variable, `X_test_tfidf`, contains the TF-IDF values for each message in the test set. Each row corresponds to a message, and each column represents a feature derived from the vocabulary.\n","\n","By using the same TF-IDF transformation as for the training data, we ensure that the model can evaluate the test data consistently, allowing for proper performance assessment.\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["X_test_tfidf = tfidf.transform(X_test['message'])"]},{"cell_type":"markdown","metadata":{},"source":["Step 4.1: Feature Scaling\n","### Initializing the Standard Scaler\n","\n","In this step, we are creating an instance of the `StandardScaler`:\n","\n","- **Purpose**: \n","  - The `StandardScaler` is used to standardize features by removing the mean and scaling to unit variance. This process is essential in many machine learning algorithms to ensure that all features contribute equally to the distance calculations.\n","\n","- **Code Explanation**:\n","  - The code `scaler = StandardScaler()` initializes a `StandardScaler` object. This object will later be used to fit and transform the feature data, allowing it to scale appropriately.\n","\n","By standardizing the features, we help improve the convergence and performance of machine learning algorithms, especially those that rely on distance metrics.\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["scaler = StandardScaler()"]},{"cell_type":"markdown","metadata":{},"source":["### Scaling Features Using StandardScaler\n","\n","In this step, we are applying standard scaling to the numerical features in the training and test datasets:\n","\n","- **Purpose**: \n","  - The goal of scaling is to standardize the features so that they have a mean of 0 and a standard deviation of 1. This ensures that the model treats all features equally, which is especially important for algorithms sensitive to the scale of the input data.\n","\n","- **Code Explanation**:\n","  - **Training Data Scaling**: \n","    - The code `X_train_scaled = scaler.fit_transform(X_train[['fingers', 'tail']])` does the following:\n","      - **fit_transform()**: \n","        - **fit**: The `StandardScaler` calculates the mean and standard deviation for the features `'fingers'` and `'tail'` in the training set.\n","        - **transform**: It then standardizes these features based on the calculated mean and standard deviation, producing a scaled version stored in `X_train_scaled`.\n","\n","  - **Test Data Scaling**: \n","    - The code `X_test_scaled = scaler.transform(X_test[['fingers', 'tail']])` applies the scaling to the test data:\n","      - **transform()**: \n","        - This method uses the mean and standard deviation calculated from the training data to standardize the test features, ensuring that the scaling is consistent between training and testing sets.\n","\n","The resulting variables, `X_train_scaled` and `X_test_scaled`, contain the scaled versions of the `'fingers'` and `'tail'` features, ready for use in machine learning models.\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["X_train_scaled = scaler.fit_transform(X_train[['fingers', 'tail']]) \n","X_test_scaled = scaler.transform(X_test[['fingers', 'tail']])"]},{"cell_type":"markdown","metadata":{},"source":["### Combining Features from TF-IDF and Scaled Data\n","\n","In this step, we are combining the features from the TF-IDF transformation and the scaled numerical features into a single feature set for both the training and test datasets:\n","\n","- **Purpose**: \n","  - The goal is to create a unified feature matrix that includes both textual and numerical data. This combined feature set can then be used as input for machine learning models, allowing them to leverage information from both types of features.\n","\n","- **Code Explanation**:\n","  - **Combining Training Data**: \n","    - The code `X_train_combined = np.hstack((X_train_tfidf.toarray(), X_train_scaled))` performs the following:\n","      - `X_train_tfidf.toarray()`: Converts the sparse TF-IDF matrix to a dense NumPy array format.\n","      - `np.hstack(...)`: Horizontally stacks the dense TF-IDF array and the scaled numerical features (`X_train_scaled`), resulting in a new feature matrix stored in `X_train_combined`.\n","\n","  - **Combining Test Data**: \n","    - The code `X_test_combined = np.hstack((X_test_tfidf.toarray(), X_test_scaled))` does the same for the test dataset:\n","      - It converts the sparse TF-IDF matrix for the test data to a dense array and horizontally stacks it with the scaled features (`X_test_scaled`), resulting in `X_test_combined`.\n","\n","The final combined feature matrices, `X_train_combined` and `X_test_combined`, can now be used as input for training and evaluating machine learning models, incorporating both textual and numerical information.\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["X_train_combined = np.hstack((X_train_tfidf.toarray(), X_train_scaled))\n","X_test_combined = np.hstack((X_test_tfidf.toarray(), X_test_scaled))"]},{"cell_type":"markdown","metadata":{},"source":["Step 5: Train a Logistic Regression model with improved regularization\n","### Initializing the Logistic Regression Classifier\n","\n","In this step, we are creating an instance of the `LogisticRegression` classifier:\n","\n","- **Purpose**: \n","  - Logistic Regression is a statistical method used for binary classification tasks. It estimates the probability that a given input belongs to a particular category based on the input features.\n","\n","- **Code Explanation**:\n","  - The code `clf = LogisticRegression(max_iter=1000, C=0.5, penalty='l2')` does the following:\n","    - **max_iter=1000**: This parameter sets the maximum number of iterations for the optimization algorithm to converge. A higher number can help ensure convergence, especially with complex datasets.\n","    - **C=0.5**: The inverse of regularization strength. Smaller values specify stronger regularization, which can prevent overfitting by penalizing large coefficients in the model.\n","    - **penalty='l2'**: This parameter specifies the type of regularization to use. 'l2' refers to L2 regularization, which adds a penalty equal to the square of the magnitude of coefficients to the loss function. This helps in reducing model complexity.\n","\n","The initialized classifier `clf` will be used later to fit the model on the training data and make predictions on the test data.\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["clf = LogisticRegression(max_iter=1000, C=0.5, penalty='l2')"]},{"cell_type":"markdown","metadata":{},"source":["### Fitting the Logistic Regression Model\n","\n","In this step, we are training the `LogisticRegression` classifier using the combined training dataset:\n","\n","- **Purpose**: \n","  - The goal is to fit the logistic regression model to the training data so that it can learn the relationship between the features and the target variable (`y_train`). Once trained, the model can make predictions on new, unseen data.\n","\n","- **Code Explanation**:\n","  - The code `clf.fit(X_train_combined, y_train)` performs the following:\n","    - **fit()**: This method trains the logistic regression model using the combined feature matrix `X_train_combined` and the corresponding labels `y_train`.\n","    - During this process, the model optimizes its parameters based on the provided training data, attempting to minimize the difference between the predicted and actual values.\n","\n","After this step, the classifier `clf` is now trained and ready to make predictions on the test dataset or new data.\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/html":["<style>#sk-container-id-1 {\n","  /* Definition of color scheme common for light and dark mode */\n","  --sklearn-color-text: black;\n","  --sklearn-color-line: gray;\n","  /* Definition of color scheme for unfitted estimators */\n","  --sklearn-color-unfitted-level-0: #fff5e6;\n","  --sklearn-color-unfitted-level-1: #f6e4d2;\n","  --sklearn-color-unfitted-level-2: #ffe0b3;\n","  --sklearn-color-unfitted-level-3: chocolate;\n","  /* Definition of color scheme for fitted estimators */\n","  --sklearn-color-fitted-level-0: #f0f8ff;\n","  --sklearn-color-fitted-level-1: #d4ebff;\n","  --sklearn-color-fitted-level-2: #b3dbfd;\n","  --sklearn-color-fitted-level-3: cornflowerblue;\n","\n","  /* Specific color for light theme */\n","  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n","  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-icon: #696969;\n","\n","  @media (prefers-color-scheme: dark) {\n","    /* Redefinition of color scheme for dark theme */\n","    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n","    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-icon: #878787;\n","  }\n","}\n","\n","#sk-container-id-1 {\n","  color: var(--sklearn-color-text);\n","}\n","\n","#sk-container-id-1 pre {\n","  padding: 0;\n","}\n","\n","#sk-container-id-1 input.sk-hidden--visually {\n","  border: 0;\n","  clip: rect(1px 1px 1px 1px);\n","  clip: rect(1px, 1px, 1px, 1px);\n","  height: 1px;\n","  margin: -1px;\n","  overflow: hidden;\n","  padding: 0;\n","  position: absolute;\n","  width: 1px;\n","}\n","\n","#sk-container-id-1 div.sk-dashed-wrapped {\n","  border: 1px dashed var(--sklearn-color-line);\n","  margin: 0 0.4em 0.5em 0.4em;\n","  box-sizing: border-box;\n","  padding-bottom: 0.4em;\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","#sk-container-id-1 div.sk-container {\n","  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n","     but bootstrap.min.css set `[hidden] { display: none !important; }`\n","     so we also need the `!important` here to be able to override the\n","     default hidden behavior on the sphinx rendered scikit-learn.org.\n","     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n","  display: inline-block !important;\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-text-repr-fallback {\n","  display: none;\n","}\n","\n","div.sk-parallel-item,\n","div.sk-serial,\n","div.sk-item {\n","  /* draw centered vertical line to link estimators */\n","  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n","  background-size: 2px 100%;\n","  background-repeat: no-repeat;\n","  background-position: center center;\n","}\n","\n","/* Parallel-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-parallel-item::after {\n","  content: \"\";\n","  width: 100%;\n","  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n","  flex-grow: 1;\n","}\n","\n","#sk-container-id-1 div.sk-parallel {\n","  display: flex;\n","  align-items: stretch;\n","  justify-content: center;\n","  background-color: var(--sklearn-color-background);\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item {\n","  display: flex;\n","  flex-direction: column;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:first-child::after {\n","  align-self: flex-end;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:last-child::after {\n","  align-self: flex-start;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:only-child::after {\n","  width: 0;\n","}\n","\n","/* Serial-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-serial {\n","  display: flex;\n","  flex-direction: column;\n","  align-items: center;\n","  background-color: var(--sklearn-color-background);\n","  padding-right: 1em;\n","  padding-left: 1em;\n","}\n","\n","\n","/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n","clickable and can be expanded/collapsed.\n","- Pipeline and ColumnTransformer use this feature and define the default style\n","- Estimators will overwrite some part of the style using the `sk-estimator` class\n","*/\n","\n","/* Pipeline and ColumnTransformer style (default) */\n","\n","#sk-container-id-1 div.sk-toggleable {\n","  /* Default theme specific background. It is overwritten whether we have a\n","  specific estimator or a Pipeline/ColumnTransformer */\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","/* Toggleable label */\n","#sk-container-id-1 label.sk-toggleable__label {\n","  cursor: pointer;\n","  display: block;\n","  width: 100%;\n","  margin-bottom: 0;\n","  padding: 0.5em;\n","  box-sizing: border-box;\n","  text-align: center;\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n","  /* Arrow on the left of the label */\n","  content: \"▸\";\n","  float: left;\n","  margin-right: 0.25em;\n","  color: var(--sklearn-color-icon);\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n","  color: var(--sklearn-color-text);\n","}\n","\n","/* Toggleable content - dropdown */\n","\n","#sk-container-id-1 div.sk-toggleable__content {\n","  max-height: 0;\n","  max-width: 0;\n","  overflow: hidden;\n","  text-align: left;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content pre {\n","  margin: 0.2em;\n","  border-radius: 0.25em;\n","  color: var(--sklearn-color-text);\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n","  /* Expand drop-down */\n","  max-height: 200px;\n","  max-width: 100%;\n","  overflow: auto;\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n","  content: \"▾\";\n","}\n","\n","/* Pipeline/ColumnTransformer-specific style */\n","\n","#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator-specific style */\n","\n","/* Colorize estimator box */\n","#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n","#sk-container-id-1 div.sk-label label {\n","  /* The background is the default theme color */\n","  color: var(--sklearn-color-text-on-default-background);\n","}\n","\n","/* On hover, darken the color of the background */\n","#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","/* Label box, darken color on hover, fitted */\n","#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator label */\n","\n","#sk-container-id-1 div.sk-label label {\n","  font-family: monospace;\n","  font-weight: bold;\n","  display: inline-block;\n","  line-height: 1.2em;\n","}\n","\n","#sk-container-id-1 div.sk-label-container {\n","  text-align: center;\n","}\n","\n","/* Estimator-specific */\n","#sk-container-id-1 div.sk-estimator {\n","  font-family: monospace;\n","  border: 1px dotted var(--sklearn-color-border-box);\n","  border-radius: 0.25em;\n","  box-sizing: border-box;\n","  margin-bottom: 0.5em;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","/* on hover */\n","#sk-container-id-1 div.sk-estimator:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Specification for estimator info (e.g. \"i\" and \"?\") */\n","\n","/* Common style for \"i\" and \"?\" */\n","\n",".sk-estimator-doc-link,\n","a:link.sk-estimator-doc-link,\n","a:visited.sk-estimator-doc-link {\n","  float: right;\n","  font-size: smaller;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1em;\n","  height: 1em;\n","  width: 1em;\n","  text-decoration: none !important;\n","  margin-left: 1ex;\n","  /* unfitted */\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-unfitted-level-1);\n","}\n","\n",".sk-estimator-doc-link.fitted,\n","a:link.sk-estimator-doc-link.fitted,\n","a:visited.sk-estimator-doc-link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","/* Span, style for the box shown on hovering the info icon */\n",".sk-estimator-doc-link span {\n","  display: none;\n","  z-index: 9999;\n","  position: relative;\n","  font-weight: normal;\n","  right: .2ex;\n","  padding: .5ex;\n","  margin: .5ex;\n","  width: min-content;\n","  min-width: 20ex;\n","  max-width: 50ex;\n","  color: var(--sklearn-color-text);\n","  box-shadow: 2pt 2pt 4pt #999;\n","  /* unfitted */\n","  background: var(--sklearn-color-unfitted-level-0);\n","  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n","}\n","\n",".sk-estimator-doc-link.fitted span {\n","  /* fitted */\n","  background: var(--sklearn-color-fitted-level-0);\n","  border: var(--sklearn-color-fitted-level-3);\n","}\n","\n",".sk-estimator-doc-link:hover span {\n","  display: block;\n","}\n","\n","/* \"?\"-specific style due to the `<a>` HTML tag */\n","\n","#sk-container-id-1 a.estimator_doc_link {\n","  float: right;\n","  font-size: 1rem;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1rem;\n","  height: 1rem;\n","  width: 1rem;\n","  text-decoration: none;\n","  /* unfitted */\n","  color: var(--sklearn-color-unfitted-level-1);\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","#sk-container-id-1 a.estimator_doc_link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","}\n","</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.5, max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(C=0.5, max_iter=1000)</pre></div> </div></div></div></div>"],"text/plain":["LogisticRegression(C=0.5, max_iter=1000)"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["clf.fit(X_train_combined, y_train)"]},{"cell_type":"markdown","metadata":{},"source":["Step 6: Evaluate the model on the test data\n","### Making Predictions with the Logistic Regression Model\n","\n","In this step, we are using the trained `LogisticRegression` classifier to make predictions on the test dataset:\n","\n","- **Purpose**: \n","  - The goal is to predict the target variable (`y_pred`) for the test set using the model that was previously trained on the training data. This allows us to evaluate the model's performance on unseen data.\n","\n","- **Code Explanation**:\n","  - The code `y_pred = clf.predict(X_test_combined)` performs the following:\n","    - **predict()**: This method takes the combined feature matrix `X_test_combined` as input and outputs the predicted class labels for each sample in the test dataset.\n","    - The resulting array `y_pred` contains the predicted labels for the species based on the features provided in `X_test_combined`.\n","\n","By generating predictions, we can assess the model's accuracy and effectiveness in classifying the test data.\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["y_pred = clf.predict(X_test_combined)"]},{"cell_type":"markdown","metadata":{},"source":["### Evaluating Model Accuracy\n","\n","In this step, we are assessing the performance of the logistic regression model by calculating its accuracy:\n","\n","- **Purpose**: \n","  - The goal is to determine how well the model performs on the test dataset by comparing the predicted labels with the actual labels. Accuracy is a common metric used to evaluate classification models.\n","\n","- **Code Explanation**:\n","  - The code `accuracy = accuracy_score(y_test, y_pred)` performs the following:\n","    - **accuracy_score()**: This function computes the accuracy of the model by comparing the actual labels (`y_test`) with the predicted labels (`y_pred`). The result is a floating-point number between 0 and 1, representing the proportion of correct predictions.\n","    - The calculated accuracy is stored in the variable `accuracy`.\n","\n","- **Printing the Accuracy**: \n","  - The code `print(f\"Model Accuracy after optimization: {accuracy * 100:.2f}%\")` outputs the accuracy as a percentage:\n","    - The accuracy is multiplied by 100 to convert it to a percentage.\n","    - The formatting `.2f` ensures that the accuracy is displayed with two decimal places.\n","\n","This output provides a clear and concise indication of how well the model is performing on the test data, allowing for assessment of its effectiveness.\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model Accuracy after optimization: 77.00%\n"]}],"source":["accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Model Accuracy after optimization: {accuracy * 100:.2f}%\")"]},{"cell_type":"markdown","metadata":{},"source":["Step 7: Save the trained model and the TF-IDF vectorizer\n","### Saving the Trained Model and Preprocessing Objects\n","\n","In this step, we are saving the trained logistic regression model, the TF-IDF vectorizer, and the scaler to disk:\n","\n","- **Purpose**: \n","  - Saving these objects allows for easy reloading and use in future predictions or evaluations without needing to retrain the model or refit the vectorizer and scaler. This is particularly useful for deployment in production environments or for sharing with others.\n","\n","- **Code Explanation**:\n","  - **Saving the Model**:\n","    - The code `joblib.dump(clf, 'logistic_regression_model.pkl')` saves the trained logistic regression model to a file named `logistic_regression_model.pkl` using the `joblib` library.\n","  - **Saving the TF-IDF Vectorizer**:\n","    - The code `joblib.dump(tfidf, 'tfidf_vectorizer.pkl')` saves the TF-IDF vectorizer to a file named `tfidf_vectorizer.pkl`. This allows for consistent text vectorization in the future using the same settings and vocabulary learned during training.\n","  - **Saving the Scaler**:\n","    - The code `joblib.dump(scaler, 'scaler.pkl')` saves the scaler object to a file named `scaler.pkl`, ensuring that the same scaling parameters can be applied to new data.\n","\n","By using `joblib.dump`, we ensure that the model and preprocessing objects can be efficiently serialized and saved, facilitating their reuse later.\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["['scaler.pkl']"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["joblib.dump(clf, 'logistic_regression_model.pkl')\n","joblib.dump(tfidf, 'tfidf_vectorizer.pkl')\n","joblib.dump(scaler, 'scaler.pkl')"]},{"cell_type":"markdown","metadata":{},"source":["Step 8: Load the saved model and vectorizer to make predictions on new data\n","### Loading the Saved Model and Preprocessing Objects\n","\n","In this step, we are loading the previously saved logistic regression model, TF-IDF vectorizer, and scaler from disk:\n","\n","- **Purpose**: \n","  - Loading these objects allows us to reuse the trained model and preprocessing settings without having to retrain or refit them. This is essential for making predictions on new data efficiently and consistently.\n","\n","- **Code Explanation**:\n","  - **Loading the Model**:\n","    - The code `loaded_model = joblib.load('logistic_regression_model.pkl')` loads the saved logistic regression model from the file `logistic_regression_model.pkl` into the variable `loaded_model`. This allows us to use the trained model for making predictions.\n","  \n","  - **Loading the TF-IDF Vectorizer**:\n","    - The code `loaded_tfidf = joblib.load('tfidf_vectorizer.pkl')` loads the TF-IDF vectorizer from the file `tfidf_vectorizer.pkl` into the variable `loaded_tfidf`. This vectorizer can then be used to transform new text data into the same format used during training.\n","\n","  - **Loading the Scaler**:\n","    - The code `loaded_scaler = joblib.load('scaler.pkl')` loads the scaler from the file `scaler.pkl` into the variable `loaded_scaler`. This scaler ensures that any new numerical features are scaled consistently with the training data.\n","\n","By loading these objects, we can seamlessly continue our workflow with the trained model and preprocessing tools, enabling predictions on new datasets.\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["loaded_model = joblib.load('logistic_regression_model.pkl')\n","loaded_tfidf = joblib.load('tfidf_vectorizer.pkl')\n","loaded_scaler = joblib.load('scaler.pkl')"]},{"cell_type":"markdown","metadata":{},"source":["Step 9: Predict species for new data (test.csv)\n","### Preparing the Test Dataset\n","\n","In this step, we are loading a new test dataset and preprocessing one of its features:\n","\n","- **Purpose**: \n","  - The goal is to prepare the test dataset for making predictions. This includes ensuring that the features in the test dataset match the format and processing of the training dataset.\n","\n","- **Code Explanation**:\n","  - **Loading the Test Dataset**:\n","    - The code `test_data = pd.read_csv('test.csv')` reads the test data from a CSV file named `test.csv` into a Pandas DataFrame called `test_data`. This DataFrame will be used for making predictions.\n","\n","  - **Preprocessing the 'tail' Feature**:\n","    - The code `test_data['tail'] = test_data['tail'].apply(lambda x: 1 if x == 'Yes' else 0)` applies a transformation to the 'tail' column in the `test_data` DataFrame:\n","      - This transformation converts the values in the 'tail' column from categorical ('Yes' or 'No') to binary numerical values (1 for 'Yes' and 0 for 'No'). \n","      - This ensures that the 'tail' feature is in the same format as it was during the training phase, allowing the model to make accurate predictions.\n","\n","By completing this preprocessing step, the `test_data` DataFrame is now ready for further processing and predictions using the trained model.\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["test_data = pd.read_csv('test.csv')\n","test_data['tail'] = test_data['tail'].apply(lambda x: 1 if x == 'Yes' else 0)"]},{"cell_type":"markdown","metadata":{},"source":["### Transforming the Test Data Using the Loaded TF-IDF Vectorizer\n","\n","In this step, we are applying the loaded TF-IDF vectorizer to the test dataset to prepare the text data for predictions:\n","\n","- **Purpose**: \n","  - The goal is to convert the text messages in the test dataset into a numerical format that the trained model can understand. This transformation is crucial for enabling the model to make predictions on the new text data.\n","\n","- **Code Explanation**:\n","  - The code `X_test_final_tfidf = loaded_tfidf.transform(test_data['message'])` performs the following:\n","    - **transform()**: This method of the TF-IDF vectorizer takes the 'message' column from the `test_data` DataFrame as input and transforms it into a TF-IDF feature matrix.\n","    - The resulting matrix, `X_test_final_tfidf`, contains the TF-IDF scores for the text messages in the test dataset, based on the vocabulary and settings learned during the training phase.\n","\n","By transforming the test data using the loaded TF-IDF vectorizer, we ensure that the text is represented in a format compatible with the trained logistic regression model, allowing for accurate predictions.\n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["X_test_final_tfidf = loaded_tfidf.transform(test_data['message'])"]},{"cell_type":"markdown","metadata":{},"source":["### Scaling the Test Data Using the Loaded Scaler\n","\n","In this step, we are applying the loaded scaler to the numerical features of the test dataset to ensure they are properly scaled:\n","\n","- **Purpose**: \n","  - The goal is to transform the numerical features in the test dataset, specifically 'fingers' and 'tail', into a standardized format. This scaling ensures that the model receives the same input format as it did during training, which is essential for accurate predictions.\n","\n","- **Code Explanation**:\n","  - The code `X_test_final_scaled = loaded_scaler.transform(test_data[['fingers', 'tail']])` performs the following:\n","    - **transform()**: This method of the scaler takes the selected columns ('fingers' and 'tail') from the `test_data` DataFrame as input and scales them based on the parameters (mean and standard deviation) learned during the fitting of the scaler on the training data.\n","    - The resulting matrix, `X_test_final_scaled`, contains the scaled values for the 'fingers' and 'tail' features.\n","\n","By scaling the test data using the loaded scaler, we ensure that the numerical features are processed in a manner consistent with the training phase, enabling the model to make reliable predictions on the test dataset.\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["X_test_final_scaled = loaded_scaler.transform(test_data[['fingers', 'tail']])"]},{"cell_type":"markdown","metadata":{},"source":["### Combining the Transformed Test Data\n","\n","In this step, we are merging the TF-IDF feature matrix and the scaled numerical features from the test dataset:\n","\n","- **Purpose**: \n","  - The goal is to create a final feature matrix that includes both the textual and numerical data. This combined matrix will be used as input for making predictions with the trained model.\n","\n","- **Code Explanation**:\n","  - The code `X_test_final_combined = np.hstack((X_test_final_tfidf.toarray(), X_test_final_scaled))` performs the following:\n","    - **np.hstack()**: This function from the NumPy library horizontally stacks the two arrays:\n","      - `X_test_final_tfidf.toarray()`: Converts the sparse matrix of TF-IDF features into a dense array format.\n","      - `X_test_final_scaled`: Contains the scaled values for the 'fingers' and 'tail' features.\n","    - The result, `X_test_final_combined`, is a 2D array that combines both the TF-IDF features and the scaled numerical features.\n","\n","By combining the transformed text data and the scaled numerical features, we ensure that the input format for the prediction phase matches the format used during training, allowing the model to make accurate predictions on the test dataset.\n"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["X_test_final_combined = np.hstack((X_test_final_tfidf.toarray(), X_test_final_scaled))"]},{"cell_type":"markdown","metadata":{},"source":["### Making Predictions on the Test Data\n","\n","In this step, we are using the loaded logistic regression model to make predictions on the combined test dataset:\n","\n","- **Purpose**: \n","  - The goal is to classify the test data based on the features prepared in the previous steps. This allows us to determine the predicted labels for the test dataset.\n","\n","- **Code Explanation**:\n","  - The code `test_pred = loaded_model.predict(X_test_final_combined)` performs the following:\n","    - **predict()**: This method of the logistic regression model takes the combined feature matrix `X_test_final_combined` as input and generates predictions.\n","    - The result, stored in `test_pred`, is an array of predicted labels for each entry in the test dataset. These labels correspond to the classes (species) that the model was trained to recognize.\n","\n","By making predictions on the test data, we can evaluate the model's performance on unseen data and assess its effectiveness in classifying new instances based on the learned patterns.\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["test_pred = loaded_model.predict(X_test_final_combined)"]},{"cell_type":"markdown","metadata":{},"source":["### Converting Predicted Labels Back to Original Species Names\n","\n","In this step, we are converting the predicted numerical labels back to their original categorical names using the label encoder:\n","\n","- **Purpose**: \n","  - The goal is to translate the model's predicted numerical labels (which represent different species) back into their original species names for better interpretability of the results.\n","\n","- **Code Explanation**:\n","  - The code `test_pred_species = le.inverse_transform(test_pred)` performs the following:\n","    - **inverse_transform()**: This method of the label encoder takes the array of predicted labels (`test_pred`) as input and maps each numerical label back to its corresponding original category (species name).\n","    - The result, stored in `test_pred_species`, is an array containing the predicted species names, making it easier to understand the model's predictions.\n","\n","By converting the predicted numerical labels back to their original species names, we enhance the readability of the predictions, allowing us to interpret and analyze the model's performance more effectively.\n"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["test_pred_species = le.inverse_transform(test_pred)"]},{"cell_type":"markdown","metadata":{},"source":["Step 10: Save the predictions to result.csv\n","### Saving the Predictions to a CSV File Without Header\n","\n","In this step, we are creating a DataFrame to store the predicted species names and then saving this DataFrame to a CSV file without including the header row:\n","\n","- **Purpose**: \n","  - The goal is to save the results of the model's predictions in a structured format for easy access and analysis later. Omitting the header can be useful when the CSV file is intended for use in applications that do not require column names.\n","\n","- **Code Explanation**:\n","  - The code `result = pd.DataFrame({'species': test_pred_species})` performs the following:\n","    - **Creating a DataFrame**: A new Pandas DataFrame named `result` is created, containing a single column 'species' with the predicted species names from `test_pred_species`.\n","  \n","  - The code `result.to_csv('result.csv', index=False, header=False)` performs the following:\n","    - **to_csv()**: This method saves the DataFrame `result` to a CSV file named `result.csv`.\n","    - The parameter `index=False` is specified to prevent Pandas from writing row indices to the CSV file.\n","    - The parameter `header=False` is specified to omit the header row from the CSV file, resulting in a file that only contains the predicted species names.\n","\n","By saving the predictions to a CSV file without a header, we ensure that the results are stored in a concise format, making it suitable for specific use cases where headers are not needed.\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["result = pd.DataFrame({'species': test_pred_species})\n","result.to_csv('result.csv', index=False, header=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Confirming the Save Operation\n","\n","In this step, we are printing a confirmation message to indicate that the predictions have been successfully saved to a CSV file:\n","\n","- **Purpose**: \n","  - The goal is to provide feedback to the user, confirming that the operation of saving the predictions has been completed successfully. This is especially useful in longer scripts where the user may want assurance that certain tasks have been executed.\n","\n","- **Code Explanation**:\n","  - The code `print(\"Predictions saved to result.csv\")` performs the following:\n","    - **print()**: This function outputs the specified message to the console.\n","    - The message informs the user that the predictions have been saved in a file named `result.csv`, indicating the completion of the previous operations.\n","\n","By providing a confirmation message, we enhance the user experience by clearly communicating the success of the task, helping users track the workflow of the script.\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Predictions saved to result.csv\n"]}],"source":["print(\"Predictions saved to result.csv\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":2}
